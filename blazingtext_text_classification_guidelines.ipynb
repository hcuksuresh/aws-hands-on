{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = 'bucket-name' # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'blazing/supervised/newlabels' #Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'blazing/supervised/training_original/filename.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "df = pd.read_csv(data_location, header=None) # df columns - label, state, covidguideline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['label', 'state', 'covidguideline']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('unique labels: \\n', list(df.label.unique())), \n",
    "print('\\n')\n",
    "print('no. of unique labels: ', len(df.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort1 = ['L1', 'L2', 'L3', 'L4', 'L5', 'L6', 'L7', 'L8', 'L9', 'L10', 'L11']\n",
    "\n",
    "sort1.sort()\n",
    "sort1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].map({'L1':1,\n",
    " 'L2':2,\n",
    " 'L3':3,\n",
    " 'L4':4,\n",
    " 'L5':5,\n",
    " 'L6':6,\n",
    " 'L7':7,\n",
    " 'L8':8,\n",
    " 'L9':9,\n",
    " 'L10':10,\n",
    " 'L11':11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  7,  8,  4,  5,  6,  9, 11, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = np.random.rand(len(df)) < 0.8\n",
    "train = df[split]\n",
    "test = df[~split]\n",
    "\n",
    "print('train samples: \\n', len(train))\n",
    "print('\\n')\n",
    "print('test samples: \\n', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_prefix = 'blazing/supervised/training_original/'\n",
    "data_path = 's3://{}/{}'.format(bucket, object_prefix)\n",
    "\n",
    "#train.to_csv('./data_new_labels/train.csv', header=None, index=False)\n",
    "#test.to_csv('./data_new_labels/test.csv', header=None, index=False)\n",
    "\n",
    "train.to_csv(data_path+'train.csv', header=None, index=False)\n",
    "test.to_csv(data_path+'test.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,businesses_reopening,\"'May 1: reopening all closed businesses. Restaurants that remained open for takeout and delivery may not reopen for dine-in until May 1.  May 15: large event venues, xa0sports and recreation venues, entertainment venues  May 29: state enters Green Phase  September xa04: state begins county-by-county risk level assessment. Requirements for businesses with respect to mitigation protocols varies by risk level.'\"\r\n",
      "2,operation_restrictions,\"'Food establishment occupancy limits: 50% of capacity in Yellow Phase; increases to 75% in Green Phase and to 100% in Blue Phase.  Additional occupancy guidelines and other operating requirements apply to personal care services, movie theaters, and fitness centers. Refer to Smart Restart industry specific protocols.'\"\r\n",
      "3,ppe_requirement,'Recommended generally. Encourage use of cloth face coverings by employees whose duties require close contact with other employees and/or the public.  Personal care services: required.'\r\n"
     ]
    }
   ],
   "source": [
    "!head data_new_labels/train.csv -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data_new_labels/classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = {} \n",
    "with open(\"data_new_labels/classes.txt\") as f:\n",
    "    for i,label in enumerate(f.readlines()):\n",
    "        index_to_label[str(i+1)] = label.strip()\n",
    "print(index_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + index_to_label[row[0]]  #Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower()))\n",
    "    cur_row.extend(nltk.word_tokenize(row[2].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.1 ms, sys: 20.5 ms, total: 42.6 ms\n",
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preprocess('data_new_labels/train.csv', 'guidelines.train', keep=1)\n",
    "        \n",
    "# Preparing the validation dataset        \n",
    "preprocess('data_new_labels/test.csv', 'guidelines.validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72.5 ms, sys: 15.6 ms, total: 88.1 ms\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='guidelines.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='guidelines.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-2\n"
     ]
    }
   ],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "print(region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 825641698319.dkr.ecr.us-east-2.amazonaws.com/blazingtext:latest (us-east-2)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BlazingText model for supervised text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the original implementation of [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf), SageMaker BlazingText provides an efficient implementation of the continuous bag-of-words (CBOW) and skip-gram architectures using Negative Sampling, on CPUs and additionally on GPU[s]. The GPU implementation uses highly optimized CUDA kernels. To learn more, please refer to [*BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs*](https://dl.acm.org/citation.cfm?doid=3146347.3146354).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides skip-gram and CBOW, SageMaker BlazingText also supports the \"Batch Skipgram\" mode, which uses efficient mini-batching and matrix-matrix operations ([BLAS Level 3 routines](https://software.intel.com/en-us/mkl-developer-reference-fortran-blas-level-3-routines)). This mode enables distributed word2vec training across multiple CPU nodes, allowing almost linear scale up of word2vec computation to process hundreds of millions of words per second. Please refer to [*Parallelizing Word2Vec in Shared and Distributed Memory*](https://arxiv.org/pdf/1604.04661.pdf) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlazingText also supports a *supervised* mode for text classification. It extends the FastText text classifier to leverage GPU acceleration using custom CUDA kernels. The model can be trained on more than a billion words in a couple of minutes using a multi-core CPU or a GPU, while achieving performance on par with the state-of-the-art deep learning text classification algorithms. For more information, please refer to the [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the following modes are supported by BlazingText on different types instances:\n",
    "\n",
    "|          Modes         \t| cbow (supports subwords training) \t| skipgram (supports subwords training) \t| batch_skipgram \t| supervised |\n",
    "|:----------------------:\t|:----:\t|:--------:\t|:--------------:\t| :--------------:\t|\n",
    "|   Single CPU instance  \t|   ✔  \t|     ✔    \t|        ✔       \t|  ✔  |\n",
    "|   Single GPU instance  \t|   ✔  \t|     ✔    \t|                \t|  ✔ (Instance with 1 GPU only)  |\n",
    "| Multiple CPU instances \t|      \t|          \t|        ✔       \t|     | |\n",
    "\n",
    "Now, let's define the SageMaker `Estimator` with resource configurations and hyperparameters to train Text Classification on *DBPedia* dataset, using \"supervised\" mode on a `c4.4xlarge` instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "guidelines_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) for the complete list of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidelines_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=100,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=20,\n",
    "                            early_stopping=True,\n",
    "                            patience=6,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyper-parameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <sagemaker.inputs.s3_input at 0x7f48dd1d5d68>,\n",
       " 'validation': <sagemaker.inputs.s3_input at 0x7f48dd1d57f0>}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-06 06:16:17 Starting - Starting the training job...\n",
      "2020-11-06 06:16:19 Starting - Launching requested ML instances......\n",
      "2020-11-06 06:17:25 Starting - Preparing the instances for training......\n",
      "2020-11-06 06:18:48 Downloading - Downloading input data...\n",
      "2020-11-06 06:19:10 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[11/06/2020 06:19:24 WARNING 140080725144960] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[11/06/2020 06:19:24 WARNING 140080725144960] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[11/06/2020 06:19:24 INFO 140080725144960] nvidia-smi took: 0.02536940574645996 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[11/06/2020 06:19:24 INFO 140080725144960] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mNumber of CPU sockets found in instance is  1\u001b[0m\n",
      "\u001b[34m[11/06/2020 06:19:24 INFO 140080725144960] Processing /opt/ml/input/data/train/guidelines.train . File size: 0.16891956329345703 MB\u001b[0m\n",
      "\u001b[34m[11/06/2020 06:19:24 INFO 140080725144960] Processing /opt/ml/input/data/validation/guidelines.validation . File size: 0.04774284362792969 MB\u001b[0m\n",
      "\u001b[34mRead 0M words\u001b[0m\n",
      "\u001b[34mNumber of words:  1221\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/guidelines.validation\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 16\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.884615\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0369  Progress: 26.20%  Million Words/sec: 6.14 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 26\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.956044\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0279  Progress: 44.10%  Million Words/sec: 5.51 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 44\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.978022\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0186  Progress: 62.82%  Million Words/sec: 5.35 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 63\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.983516\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0092  Progress: 81.59%  Million Words/sec: 5.27 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 81\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.983516\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: -0.0000  Progress: 100.03%  Million Words/sec: 5.20 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 100\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.983516\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 5.18 #####\n",
      "\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 5.18\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 0.52\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.9987\u001b[0m\n",
      "\u001b[34mNumber of train examples: 743\n",
      "\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.9835\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 182\u001b[0m\n",
      "\n",
      "2020-11-06 06:19:41 Uploading - Uploading generated training model\n",
      "2020-11-06 06:19:41 Completed - Training job completed\n",
      "Training seconds: 53\n",
      "Billable seconds: 53\n"
     ]
    }
   ],
   "source": [
    "guidelines_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same type of instance that we used to train. Because instance endpoints will be up and running for long, it's advisable to choose a cheaper instance for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "Using already existing model: blazingtext-2020-11-06-06-16-17-570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "text_classifier = guidelines_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge', endpoint_name = 'end-point-name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use the following deployment script in a separt notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use JSON format for inference\n",
    "BlazingText supports `application/json` as the content-type for inference. The payload should contain a list of sentences with the key as \"**instances**\" while being passed to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "#import json\n",
    "#import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "#role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RealTimePredictor using SageMaker SDK\n",
    "# Specify Your Endpoint Name\n",
    "endpoint_name = 'end-point-name'\n",
    "\n",
    "predictor = sagemaker.predictor.RealTimePredictor(endpoint=endpoint_name,\n",
    "                                                 sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    text = text.split()\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__GD-102\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.663482666015625\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#single inference #ppe\n",
    "sentences = [\"Recommended generally. Encourage use of cloth face coverings by employees whose duties require close contact with other employees and/or the public.  Personal care services: required.\"]\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "#print(tokenized_sentences)\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = predictor.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__GD-101\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.9612232446670532\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#single inference #operations restrictions\n",
    "sentences = [\"Food establishment occupancy limits: 50% of capacity in Yellow Phase; increases to 75% in Green Phase and to 100% in Blue Phase.  Additional occupancy guidelines and other operating requirements apply to personal care services, movie theaters, and fitness centers. Refer to Smart Restart industry specific protocols.\"]\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "#print(tokenized_sentences)\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = predictor.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__GD-100\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.5057657957077026\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#single inference #operations restrictions #business reopen\n",
    "sentences = [\"May 1: reopening all closed businesses. Restaurants that remained open for takeout and delivery may not reopen for dine-in until May 1.  May 15: large event venues, xa0sports and recreation venues, entertainment venues  May 29: state enters Green Phase  September xa04: state begins county-by-county risk level assessment. Requirements for businesses with respect to mitigation protocols varies by risk level.\"]\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "#print(tokenized_sentences)\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = predictor.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))              \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "guidelines = pd.read_csv('testfile_new2.csv', header=None)\n",
    "guidelines.columns = ['id','l_id','datasource_id','source_day_id','News','Links','s_upsertTimestamp','w_upsertTimestamp']\n",
    "guidelines['News'] = guidelines['News'].astype(str)\n",
    "guidelines['News'] = guidelines['News'].apply(lambda x: x.lower())\n",
    "guidelines['News'] = guidelines['News'].apply(lambda x: clean(x))\n",
    "guidelines.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_lines = guidelines[['id', 'News']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payload created\n"
     ]
    }
   ],
   "source": [
    "sentences = list(guide_lines.News)\n",
    "tokenized_sentences = sentences\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "print('payload created')\n",
    "response = predictor.predict(json.dumps(payload))\n",
    "predictions = json.loads(response)\n",
    "label_list = []\n",
    "for i in range(len(sentences)):\n",
    "       label_list.append(predictions[i]['label'][0].split('__label__')[1])\n",
    "final_df = pd.DataFrame(label_list, columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head() #only predicted column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "guide_lines['label'] = final_df['label']\n",
    "final_df = guide_lines[['id', 'label']]\n",
    "final_df.columns = ['table_id', 'label']\n",
    "\n",
    "final_df['table_name'] = 'covid_data.newstable'\n",
    "final_df['algorithm'] = 'blazing text'\n",
    "final_df['label_type'] = 'categorization'\n",
    "final_df['w_upsert_timestamp'] = pd.Series([dt.datetime.now()] * len(final_df))\n",
    "    \n",
    "final_df = final_df[['table_name', 'algorithm', 'table_w_id', 'label_type', 'label', 'w_upsert_timestamp']]\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to local\n",
    "final_df.to_csv('predicted_Guidelinesfile_push_to_redshift_new_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write S3 bucket\n",
    "final_df.to_csv('s3://bucket/blazing/outputfile/predicted_file_push_to_redshift.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.delete_endpoint(text_classifier.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
